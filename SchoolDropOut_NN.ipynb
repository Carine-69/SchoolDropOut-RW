{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWqXPnsmcxNYtByQtkSY0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carine-69/SchoolDropOut-RW/blob/main/SchoolDropOut_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "1HRDepBo7iY2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGxXJ5JI7zF4",
        "outputId": "9cb17833-5bdd-4ebb-9429-d2f57cd0b0b5"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def load_clean_data(filename, test_size=0.2, random_state=42):\n",
        "  # Load data using numpy as it handles mixed types and headers better\n",
        "  df = np.genfromtxt('/content/drive/MyDrive/ML-datasets/SchoolDropOut.csv', delimiter = ',', dtype=float, filling_values = np.nan)\n",
        "\n",
        "  # Impute missing values with the mean of the respective column.\n",
        "  for i in range(df.shape[1]):\n",
        "        col = df[:, i]\n",
        "        # Find non-NaN values to calculate the mean\n",
        "        non_nan_indices = ~np.isnan(col)\n",
        "        # Check if there are any non-NaN values in the column\n",
        "        if np.any(non_nan_indices):\n",
        "            mean_val = np.mean(col[non_nan_indices])\n",
        "            # Replace NaN values with the calculated mean\n",
        "            col[np.isnan(col)] = mean_val\n",
        "        else:\n",
        "            # If the entire column is NaN, fill with 0 or a placeholder\n",
        "            col[np.isnan(col)] = 0\n",
        "\n",
        "  return df # Added missing return statement"
      ],
      "metadata": {
        "id": "gy2IACq5RkRa"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df, test_size=0.2, random_state=42):\n",
        "    #separate features and labels\n",
        "    X = df[:, :-1]\n",
        "    y = df[:, -1].astype(int)\n",
        "\n",
        "    # Define the sentinel value for invalid integers\n",
        "    invalid_int_sentinel = -9223372036854775808\n",
        "\n",
        "    # Remove rows where y is NaN or the invalid integer sentinel\n",
        "    nan_mask = np.isnan(y)\n",
        "    invalid_int_mask = (y == invalid_int_sentinel)\n",
        "    combined_mask = nan_mask | invalid_int_mask\n",
        "\n",
        "    X = X[~combined_mask]\n",
        "    y = y[~combined_mask]\n",
        "\n",
        "    # Check if y is empty after cleaning\n",
        "    if y.size == 0:\n",
        "        print(\"Warning: Target variable 'y' is empty after cleaning. No valid data remains.\")\n",
        "        # Return empty arrays or None values\n",
        "        return np.array([]), np.array([]), np.array([]), np.array([]), 0\n",
        "\n",
        "\n",
        "    #convert integers into one hot encoded format\n",
        "    num_classes = len(np.unique(y))\n",
        "    y_one_hot = np.eye(num_classes)[y].T\n",
        "\n",
        "    #split data into training and testing\n",
        "    np.random.seed(random_state)\n",
        "    indices = np.random.permutation(X.shape[0])\n",
        "    test_size = int(X.shape[0] * test_size)\n",
        "    test_indices = indices[:test_size]\n",
        "    train_indices = indices[test_size:]\n",
        "\n",
        "    X_train = X[train_indices]\n",
        "    y_train = y_one_hot[:, train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_test = y_one_hot[:, test_indices]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, num_classes # Added the missing return statement\n",
        "    # define activation functions\n",
        "def relu(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(a):\n",
        "  return (a>0).astype(np.float64)\n",
        "\n",
        "def softmax(z):\n",
        "  output = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "  return output / np.sum(output, axis=0, keepdims=True)\n",
        "\n",
        "  # loss function\n",
        "  def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.mean(y_true * np.log(y_pred + 1e-7))\n",
        "\n",
        "    # define layers\n",
        "class ThreeLayerNN:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden1_size = hidden_size\n",
        "    self.hidden2_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "      #initializer parameters\n",
        "  def initialize_parameters(self):\n",
        "        np.random.seed(1)\n",
        "        params = {\n",
        "            'W1': np.random.randn(self.hidden1_size, self.input_size) * 0.01,\n",
        "            'b1': np.zeros((self.hidden1_size, 1)),\n",
        "            'W2': np.random.randn(self.hidden2_size, self.hidden1_size) * 0.01,\n",
        "            'b2': np.zeros((self.hidden2_size, 1)),\n",
        "            'W3': np.random.randn(self.output_size, self.hidden2_size) * 0.01,\n",
        "            'b3': np.zeros((self.output_size, 1))\n",
        "        }\n",
        "        return params\n",
        "\n",
        "          # forward propagation\n",
        "  def forward_propagation(self, x):\n",
        "        params = self.params\n",
        "        # layer 1\n",
        "        l1 = np.dot(params['W1'], x) + params['b1']\n",
        "        a1 = relu(l1)\n",
        "        l2 = np.dot(params['W2'], a1) + params['b2']\n",
        "        a2 = relu(l2)\n",
        "        l3 = np.dot(params['W3'], a2) + params['b3']\n",
        "        a3 = softmax(l3)\n",
        "        cache = (l1, a1, l2, a2, l3, a3)\n",
        "        return a3, cache\n",
        "\n",
        "          # backpropagation\n",
        "  def backpropagation(self, x, y, l1, a1, l2, a2, l3, a3):\n",
        "        params = self.params\n",
        "        m = x.shape[1]\n",
        "\n",
        "        #outer layer\n",
        "        dl3 = a3 - y\n",
        "        dW3 = (1/m) * np.dot(dl3, a2.T)\n",
        "        db3 = (1/m) * np.sum(dl3, axis=1, keepdims=True)\n",
        "\n",
        "        #second hidden layer\n",
        "        dl2 = np.dot(params['W3'].T, dl3) * relu_derivative(a2)\n",
        "        dW2 = (1/m) * np.dot(dl2, a1.T)\n",
        "        db2 = (1/m) * np.sum(dl2, axis=1, keepdims=True)\n",
        "\n",
        "        #first hidden layer\n",
        "        dl1 = np.dot(params['W2'].T, dl2) * relu_derivative(a1)\n",
        "        dW1 = (1/m) * np.dot(dl1, x.T)\n",
        "        db1 = (1/m) * np.sum(dl1, axis=1, keepdims=True)\n",
        "\n",
        "        return dW1, db1, dW2, db2, dW3, db3\n",
        "\n",
        "          #update parameters\n",
        "  def upt_params(self, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
        "        params = self.params\n",
        "        params['W1'] -= learning_rate * dW1\n",
        "        params['b1'] -= learning_rate * db1\n",
        "        params['W2'] -= learning_rate * dW2\n",
        "        params['b2'] -= learning_rate * db2\n",
        "        params['W3'] -= learning_rate * dW3\n",
        "        params['b3'] -= learning_rate * db3\n",
        "\n",
        "          #train using  mini_batches\n",
        "  def train(self,x_train, y_train,x_val, y_val, epochs, learning_rate, batch_size):\n",
        "       m_train = x_train.shape[1]\n",
        "       m_val = x_val.shape[1]\n",
        "       train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "       for epoch in range(epochs): #batches\n",
        "        permutation = np.random.permutation(m_train)\n",
        "        x_train = x_train[:, permutation]\n",
        "        y_train = y_train[:, permutation]\n",
        "\n",
        "        # forward training and pdating parameters\n",
        "        for j in range(0, m_train, batch_size):\n",
        "          x_batch = x_train[:, j:j+batch_size]\n",
        "          y_batch = y_train[:, j:j+batch_size]\n",
        "          y_pred_batch, cache = self.forward_propagation(x_batch)\n",
        "          dW1, db1, dW2, db2, dW3, db3 = self.backpropagation(x_batch, y_batch, *cache)\n",
        "          self.upt_params(dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
        "\n",
        "        # record epochs\n",
        "        y_pred_train, _ = self.forward_propagation(x_train)\n",
        "        train_acc = self.calculate_accuracy(y_train, y_pred_train)\n",
        "        train_loss = cross_entropy_loss(y_train, y_pred_train)\n",
        "\n",
        "        y_pred_val, _ = self.forward_propagation(x_val)\n",
        "        val_acc = self.calculate_accuracy(y_val, y_pred_val)\n",
        "        va_loss = cross_entropy_loss(y_val, y_pred_val)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(va_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "          print(f'epoch {epoch}: train loss = {train_loss:.4f}, val loss = {va_loss:.4f}, train acc = {train_acc:.4f}, val acc = {val_acc:.4f}')\n",
        "\n",
        "       return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "  def predict(self, x):\n",
        "    y_pred, _ = self.forward_propagation(x)\n",
        "    return np.argmax(y_pred, axis=0)\n",
        "\n",
        "  def calculate_accuracy(self, y_true, y_pred):\n",
        "    y_true = np.argmax(y_true, axis=0)\n",
        "    y_pred = np.argmax(y_pred, axis=0)\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "    # Load the data\n",
        "data = load_clean_data('/content/drive/MyDrive/ML-datasets/SchoolDropOut.csv')\n",
        "\n",
        "# Preprocess it into train/test sets\n",
        "X_train, y_train, X_test, y_test, num_classes = preprocess_data(data)\n",
        "\n",
        "# Transpose X so it matches the expected input shape in your NN (features x samples)\n",
        "X_train = X_train.T\n",
        "X_test = X_test.T"
      ],
      "metadata": {
        "id": "jJJj8-KVHvIt"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define activation functions\n",
        "def relu(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(a):\n",
        "  return (a>0).astype(np.float64)\n",
        "\n",
        "def softmax(z):\n",
        "  output = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "  return output / np.sum(output, axis=0, keepdims=True)\n",
        "\n",
        "# loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "  return -np.mean(y_true * np.log(y_pred + 1e-7))\n",
        "\n",
        "# define layers\n",
        "class ThreeLayerNN:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden1_size = hidden_size\n",
        "    self.hidden2_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "      #initializer parameters\n",
        "  def initialize_parameters(self):\n",
        "        np.random.seed(1)\n",
        "        params = {\n",
        "            'W1': np.random.randn(self.hidden1_size, self.input_size) * 0.01,\n",
        "            'b1': np.zeros((self.hidden1_size, 1)),\n",
        "            'W2': np.random.randn(self.hidden2_size, self.hidden1_size) * 0.01,\n",
        "            'b2': np.zeros((self.hidden2_size, 1)),\n",
        "            'W3': np.random.randn(self.output_size, self.hidden2_size) * 0.01,\n",
        "            'b3': np.zeros((self.output_size, 1))\n",
        "        }\n",
        "        return params\n",
        "\n",
        "          # forward propagation\n",
        "  def forward_propagation(self, x):\n",
        "        params = self.params\n",
        "        # layer 1\n",
        "        l1 = np.dot(params['W1'], x) + params['b1']\n",
        "        a1 = relu(l1)\n",
        "        l2 = np.dot(params['W2'], a1) + params['b2']\n",
        "        a2 = relu(l2)\n",
        "        l3 = np.dot(params['W3'], a2) + params['b3']\n",
        "        a3 = softmax(l3)\n",
        "        cache = (l1, a1, l2, a2, l3, a3)\n",
        "        return a3, cache\n",
        "\n",
        "          # backpropagation\n",
        "  def backpropagation(self, x, y, l1, a1, l2, a2, l3, a3):\n",
        "        params = self.params\n",
        "        m = x.shape[1]\n",
        "\n",
        "        #outer layer\n",
        "        dl3 = a3 - y\n",
        "        dW3 = (1/m) * np.dot(dl3, a2.T)\n",
        "        db3 = (1/m) * np.sum(dl3, axis=1, keepdims=True)\n",
        "\n",
        "        #second hidden layer\n",
        "        dl2 = np.dot(params['W3'].T, dl3) * relu_derivative(a2)\n",
        "        dW2 = (1/m) * np.dot(dl2, a1.T)\n",
        "        db2 = (1/m) * np.sum(dl2, axis=1, keepdims=True)\n",
        "\n",
        "        #first hidden layer\n",
        "        dl1 = np.dot(params['W2'].T, dl2) * relu_derivative(a1)\n",
        "        dW1 = (1/m) * np.dot(dl1, x.T)\n",
        "        db1 = (1/m) * np.sum(dl1, axis=1, keepdims=True)\n",
        "\n",
        "        return dW1, db1, dW2, db2, dW3, db3\n",
        "\n",
        "          #update parameters\n",
        "  def upt_params(self, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
        "        params = self.params\n",
        "        params['W1'] -= learning_rate * dW1\n",
        "        params['b1'] -= learning_rate * db1\n",
        "        params['W2'] -= learning_rate * dW2\n",
        "        params['b2'] -= learning_rate * db2\n",
        "        params['W3'] -= learning_rate * dW3\n",
        "        params['b3'] -= learning_rate * db3\n",
        "\n",
        "          #train using  mini_batches\n",
        "  def train(self,x_train, y_train,x_val, y_val, epochs, learning_rate, batch_size):\n",
        "       m_train = x_train.shape[1]\n",
        "       m_val = x_val.shape[1]\n",
        "       train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "       for epoch in range(epochs): #batches\n",
        "        permutation = np.random.permutation(m_train)\n",
        "        x_train = x_train[:, permutation]\n",
        "        y_train = y_train[:, permutation]\n",
        "\n",
        "        # forward training and pdating parameters\n",
        "        for j in range(0, m_train, batch_size):\n",
        "          x_batch = x_train[:, j:j+batch_size]\n",
        "          y_batch = y_train[:, j:j+batch_size]\n",
        "          y_pred_batch, cache = self.forward_propagation(x_batch)\n",
        "          dW1, db1, dW2, db2, dW3, db3 = self.backpropagation(x_batch, y_batch, *cache)\n",
        "          self.upt_params(dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
        "\n",
        "        # record epochs\n",
        "        y_pred_train, _ = self.forward_propagation(x_train)\n",
        "        train_acc = self.calculate_accuracy(y_train, y_pred_train)\n",
        "        train_loss = cross_entropy_loss(y_train, y_pred_train)\n",
        "\n",
        "        y_pred_val, _ = self.forward_propagation(x_val)\n",
        "        val_acc = self.calculate_accuracy(y_val, y_pred_val)\n",
        "        va_loss = cross_entropy_loss(y_val, y_pred_val)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(va_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "          print(f'epoch {epoch}: train loss = {train_loss:.4f}, val loss = {va_loss:.4f}, train acc = {train_acc:.4f}, val acc = {val_acc:.4f}')\n",
        "\n",
        "       return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "  def predict(self, x):\n",
        "    y_pred, _ = self.forward_propagation(x)\n",
        "    return np.argmax(y_pred, axis=0)\n",
        "\n",
        "  def calculate_accuracy(self, y_true, y_pred):\n",
        "    y_true = np.argmax(y_true, axis=0)\n",
        "    y_pred = np.argmax(y_pred, axis=0)\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# Define the compute_metrics function and move it to a proper scope\n",
        "def compute_metrics(cm):\n",
        "    precision = np.zeros(cm.shape[0])\n",
        "    recall = np.zeros(cm.shape[0])\n",
        "    f1 = np.zeros(cm.shape[0])\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        tp = cm[i, i]\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        fn = cm[i, :].sum() - tp\n",
        "\n",
        "        precision[i] = tp / (tp + fp + 1e-7)\n",
        "        recall[i] = tp / (tp + fn + 1e-7)\n",
        "        f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i] + 1e-7)\n",
        "\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Experiment 1\n",
        "nn1 = ThreeLayerNN(input_size=X_train.shape[0], hidden_size=32, output_size=num_classes)\n",
        "nn1.params = nn1.initialize_parameters()\n",
        "train_losses1, val_losses1, train_accs1, val_accs1 = nn1.train(\n",
        "    X_train, y_train, X_test, y_test, epochs=300, learning_rate=0.01, batch_size=64\n",
        ")\n",
        "\n",
        "# Experiment 2\n",
        "nn2 = ThreeLayerNN(input_size=X_train.shape[0], hidden_size=64, output_size=num_classes)\n",
        "nn2.params = nn2.initialize_parameters()\n",
        "train_losses2, val_losses2, train_accs2, val_accs2 = nn2.train(\n",
        "    X_train, y_train, X_test, y_test, epochs=300, learning_rate=0.005, batch_size=64\n",
        ")\n",
        "\n",
        "print(\"Experiment 1:\")\n",
        "for i in range(0, 300, 50):\n",
        "    print(f\"Epoch {i}: Train Loss={train_losses1[i]:.4f}, Val Loss={val_losses1[i]:.4f}, Train Acc={train_accs1[i]:.4f}, Val Acc={val_accs1[i]:.4f}\")\n",
        "\n",
        "print(\"\\nExperiment 2:\")\n",
        "for i in range(0, 300, 50):\n",
        "    print(f\"Epoch {i}: Train Loss={train_losses2[i]:.4f}, Val Loss={val_losses2[i]:.4f}, Train Acc={train_accs2[i]:.4f}, Val Acc={val_accs2[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKRs6V2xw5ey",
        "outputId": "bd1a7188-8d38-4a01-8b71-140a6aa48eef"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: train loss = -0.0000, val loss = -0.0000, train acc = 1.0000, val acc = 1.0000\n",
            "epoch 100: train loss = -0.0000, val loss = -0.0000, train acc = 1.0000, val acc = 1.0000\n",
            "epoch 200: train loss = -0.0000, val loss = -0.0000, train acc = 1.0000, val acc = 1.0000\n",
            "epoch 0: train loss = -0.0000, val loss = -0.0000, train acc = 1.0000, val acc = 1.0000\n",
            "epoch 100: train loss = -0.0000, val loss = -0.0000, train acc = 1.0000, val acc = 1.0000\n",
            "epoch 200: train loss = -0.0000, val loss = -0.0000, train acc = 1.0000, val acc = 1.0000\n",
            "Experiment 1:\n",
            "Epoch 0: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 50: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 100: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 150: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 200: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 250: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "\n",
            "Experiment 2:\n",
            "Epoch 0: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 50: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 100: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 150: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 200: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n",
            "Epoch 250: Train Loss=-0.0000, Val Loss=-0.0000, Train Acc=1.0000, Val Acc=1.0000\n"
          ]
        }
      ]
    }
  ]
}